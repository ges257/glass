# CHALLENGES.md

## The ML Journey: From Pure Models to Production Hybrid

This document chronicles the technical challenges encountered while building GLASS, and the solutions that emerged.

---

## Challenge 1: Pixel-Coordinate Fragility

### The Problem

Early models learned pixel-level patterns like "if x = 234.5, then column boundary." This worked perfectly on training data but failed catastrophically on new PDFs.

### What Happened

- **Model 6 (Edge GNN)**: Trained on 10,000 synthetic PDFs
- **Training Accuracy**: 86.6% on K-column prediction
- **Real-World Result**: Predicted 40-88 columns on tables with 7-12 columns

The model memorized pixel coordinates from the training distribution. When real-world PDFs had different margins, fonts, or layouts, predictions were meaningless.

### Solution

**VLM semantic understanding** - A Vision-Language Model processes the page visually and understands "this looks like a date column" without relying on pixel coordinates.

The VLM generalizes across layouts because it understands *what* columns are, not *where* they were in training data.

---

## Challenge 2: Synthetic ≠ Real

### The Problem

Models trained on synthetic PDFs achieved 98%+ accuracy. On real financial documents, they failed completely.

### What Happened

| Metric | Synthetic | Real |
|--------|-----------|------|
| Model 1 (Region) | 99.92% | Worked |
| Model 2 (Row-Type) | 99.31% | Worked |
| Model 6 (Edge GNN) | 86.6% | **40-88 columns** (expected 7-12) |

Model 6 was the critical component for column detection. Its failure on real data meant the pure-ML pipeline couldn't work.

### Root Cause

Synthetic data was *too clean*:
- Perfect font rendering
- Consistent margins
- No scanning artifacts
- Uniform column spacing

Real data had:
- Variable fonts and sizes
- Scanned document quality issues
- Merged cells and multi-line headers
- 14+ different vendor layouts

### Solution

**Human-in-the-loop refinement** - Instead of demanding perfect automation, accept that the system proposes and humans refine. This is faster than manual extraction and more reliable than failed automation.

---

## Challenge 3: CNN Couldn't Learn Boundaries (F1 = 0.003)

### The Problem

Model 4 attempted to learn column boundary prediction using a 1D CNN trained on 5-channel features.

### What Happened

```
Training Progress:
- Train loss: 0.95 → 0.27 (good convergence)
- Validation F1: 0.003 (essentially random)
```

The CNN learned to minimize training loss but couldn't generalize. Its predictions on validation data were no better than guessing.

### Root Cause

The CNN was trained on *pseudo-labels* generated by a heuristic. It was trying to learn the heuristic, not the underlying pattern. But the heuristic already worked - there was nothing for the CNN to improve.

### Solution

**Use the heuristic directly** - Model 4's heuristic (ruling lines + gap detection) works on real data. Mean of 8.5 boundaries per page on 62 table pages. The failed CNN was replaced with the working heuristic.

---

## Challenge 4: Token Classification Was the Wrong Problem

### The Problem

Model 3 was designed to classify tokens into semantic categories: AMOUNT, DATE, VENDOR, CHECK_NUMBER, etc.

### What Happened

Directory structure was created. 659,712 tokens from 5,000 PDFs were prepared. Training scripts were never written.

### Root Cause

During development, it became clear that **token classification doesn't solve the extraction problem**. Knowing that "$1,234.56" is an AMOUNT doesn't tell you which column it belongs to. The problem is *structural*, not *semantic*.

### Solution

**Spatial templates** - Define column boundaries in normalized 0-1 coordinates. The template says "column 3 is from x=0.45 to x=0.60" regardless of what tokens are in it.

---

## Challenge 5: 14+ Vendor Layouts

### The Problem

Financial documents come from many different vendors. Each vendor has its own layout:

- Different column orders
- Different header styles
- Different table widths
- Different page structures

### What Happened

A single model couldn't generalize across all layouts. Attempts to train "one model to rule them all" produced mediocre results on everything.

### Solution

**Template-based extraction** with three key properties:

1. **First document**: System proposes, human refines, template saved
2. **Similar documents**: Template applies automatically
3. **New layout**: New template created (5-minute process)

This trades "perfect generalization" for "practical coverage." 90% of documents match an existing template; 10% need new templates.

---

## Summary: From Pure ML to Production Hybrid

| Approach | Result |
|----------|--------|
| Pure ML (Models 1-6) | 98% synthetic, failed on real |
| Learned CNN | F1 = 0.003 |
| Token classification | Wrong problem formulation |
| **VLM + Heuristics + Human** | **97% field-level accuracy** |

The production system combines:
- **VLM**: Semantic column detection
- **Heuristics**: Boundary proposals and validation
- **Human-in-the-loop**: Refinement and template saving
- **Templates**: Zero-cost reuse on similar documents

This hybrid approach succeeded where pure ML failed.

---

## Timeline

| Date | Event |
|------|-------|
| Dec 1-14, 2025 | Trained Models 1-6 on synthetic data |
| Dec 15, 2025 | Model 6 failed on real data (40-88 columns) |
| Dec 15, 2025 | Discovered Model 4 heuristic works (8.5 boundaries/page) |
| Dec 18, 2025 | Proposed grid-normalized coordinate system |
| Dec 19, 2025 | Pivoted to VLM-first with human adjustment |
| Dec 20-31, 2025 | Built v9/v10 hybrid system |
| Jan 2026 | Production deployment, 97% accuracy |

---

*The lesson: High accuracy on synthetic data does not guarantee real-world capability. Production systems need robust fallbacks, human oversight, and graceful degradation.*
